{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentes en LangGraph con Gemini\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caso 1 LLM estructurado\n",
        "\n",
        "<img src=\"https://langchain-ai.github.io/langgraph/tutorials/workflows/img/augmented_llm.png\" alt=\"gate\" width=\"600\"/>\n",
        "\n",
        "Este primer ejemplo muestra cómo utilizar LangGraph y el modelo Gemini de Google para crear un agente conversacional que genere consultas de búsqueda optimizadas y justificaciones usando salidas estructuradas con Pydantic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitarás una API key de Google AI Studio para usar Gemini. Puedes obtenerla en: https://makersuite.google.com/app/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Declaración y llamado de la key del modelo LLM\n",
        "# use esta función para definir la key del modelo LLM si tuvo problemas con el archivo .env.json o create_env.py\n",
        "def _set_env(var: str):\n",
        "   if not os.environ.get(var):\n",
        "       os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"GOOGLE_API_KEY\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 1: Definir un esquema de salida estructurada con Pydantic\n",
        "Usamos Pydantic para definir una clase que representa la estructura de la respuesta que queremos obtener del modelo de lenguaje.\n",
        "En este caso, la clase `SearchQuery` tiene dos campos: `search_query` (la consulta optimizada para búsqueda web) y `justification` (la justificación de por qué esa consulta es relevante para la petición del usuario).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "    search_query: str = Field(None, description=\"Consulta optimizada para búsqueda web.\")\n",
        "    justification: str = Field(\n",
        "        None, description=\"Por qué esta consulta es relevante para la petición del usuario.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2: Configurar el modelo LLM para que devuelva una salida estructurada\n",
        "Aquí, usamos el método `with_structured_output` para indicarle al modelo que queremos que su respuesta siga el esquema definido por la clase `SearchQuery`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "structured_llm = llm.with_structured_output(SearchQuery)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 3: Invocar el modelo con una pregunta y obtener la respuesta estructurada\n",
        "Ahora, hacemos una pregunta al modelo y recibimos la respuesta siguiendo el esquema definido anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SearchQuery(search_query='relación entre la puntuación de calcio de la TC y el colesterol alto', justification='El usuario pregunta por la relación entre la puntuación de calcio de la TC y el colesterol alto.')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = structured_llm.invoke(\"¿Cómo se relaciona la puntuación de calcio en la tomografía computarizada con el colesterol alto?\")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 4: Definir una herramienta personalizada\n",
        "Ahora vamos a definimos una función llamada `multiply` que multiplica dos números. Esta función se puede usar como una herramienta que el modelo puede invocar cuando lo considere necesario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"\n",
        "    Multiplica dos números float y devuelve el resultado. \n",
        "    Ejemplo: multiplicación de a=4.0 por b=2.0 retorna 8.0\n",
        "\n",
        "    Args:\n",
        "        a (float): Primer número a multiplicar.\n",
        "        b (float): Segundo número a multiplicar.\n",
        "\n",
        "    Returns:\n",
        "        float: El resultado de multiplicar a por b.\n",
        "    \"\"\"\n",
        "    result = a*b\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 5: Integrar la herramienta al modelo LLM\n",
        "Usamos el método `bind_tools` para agregar la función `multiply` como una herramienta disponible para el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools([multiply])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 6: Invocar el modelo para que utilice la herramienta\n",
        "Hacemos una pregunta que requiere el uso de la herramienta de multiplicación. El modelo detecta la intención y llama a la función `multiply`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"b\": 3.0, \"a\": 5.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-pro-002', 'safety_ratings': []} id='run--d3e84e47-5396-4fd4-826e-87489bd849d5-0' tool_calls=[{'name': 'multiply', 'args': {'b': 3.0, 'a': 5.0}, 'id': '78903932-8f35-42f2-8561-929aaeb840f5', 'type': 'tool_call'}] usage_metadata={'input_tokens': 71, 'output_tokens': 5, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "msg = llm_with_tools.invoke(\"Cúanto es la multiplicación de 5.0 por 3.0?\")\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 7: Obtener la llamada a la herramienta realizada por el modelo\n",
        "Finalmente, podemos ver los detalles de la llamada a la herramienta que realizó el modelo con `msg.tool_calls`. Incluso podemos acceder a los argumentos que ha logrado extraer el LLM y ejecutar manualmente la función\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'b': 3.0, 'a': 5.0},\n",
              "  'id': '01df873b-8fbe-431a-95b3-ae5c3184b275',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Acceder a los argumentos de la tool\n",
        "msg.tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argumento a:  5.0 argumento b:  3.0\n",
            "resultado:  15.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Si tu mensaje es 'msg'\n",
        "if msg.tool_calls:\n",
        "    for tool_call in msg.tool_calls:\n",
        "        if tool_call['name'] == 'multiply':\n",
        "            # Ejecutar la función manualmente\n",
        "            a= tool_call['args']['a']\n",
        "            b= tool_call['args']['b']\n",
        "            print (\"argumento a: \",a, \"argumento b: \", b)\n",
        "            print (\"resultado: \", multiply(a,b))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kernel-lg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
